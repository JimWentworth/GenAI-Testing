<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>AI Models: Four Lenses — Overview • Table • Quiz</title>
<style>
  :root{
    --uiuc-navy:#13294B; --uiuc-orange:#E84A27;
    --ink:#1f2937; --muted:#6b7280; --bg:#f8fafc; --card:#ffffff; --ring:#e5e7eb;
    --blue:#2563eb; --green:#16a34a; --red:#dc2626; --amber:#f59e0b; --radius:16px;
  }
  *{box-sizing:border-box}
  html,body{margin:0;background:var(--bg);color:var(--ink);font:16px/1.5 system-ui,-apple-system,Segoe UI,Roboto,Inter,Arial,sans-serif}
  a{color:var(--uiuc-navy);text-decoration:none} a:hover{text-decoration:underline}
  .wrap{max-width:1200px;margin:24px auto 32px;padding:0 16px}
  header h1{margin:0 0 6px 0;font-size:28px;color:var(--uiuc-navy)}
  header p{margin:0 0 12px 0;color:var(--muted)}

  /* Tabs */
  .tabs{display:flex;gap:8px;flex-wrap:wrap;border-bottom:1px solid var(--ring);margin:12px 0 16px}
  .tab{border:1px solid var(--ring);border-bottom:none;border-radius:12px 12px 0 0;background:#fff;padding:10px 14px;cursor:pointer}
  .tab.active{border-color:var(--uiuc-navy);color:var(--uiuc-navy);box-shadow:0 -1px 0 #fff inset}
  .panel{display:none}
  .panel.active{display:block}
  /* Default visible if JS doesn't run */
  .panel.default-visible{display:block}

  .card{background:var(--card);border:1px solid var(--ring);border-radius:16px;box-shadow:0 1px 2px rgb(0 0 0 / 6%)}
  .card h2{margin:0;padding:16px 16px 0 16px;font-size:18px}
  .sub{padding:0 16px 12px;color:var(--muted);font-size:14px}

  /* Overview grid */
  .grid{display:grid;grid-template-columns:repeat(2,minmax(0,1fr));gap:16px}
  @media (max-width:900px){.grid{grid-template-columns:1fr}}
  .lenscard{padding:12px 16px;border-left:6px solid transparent}
  .lenscard .row{display:flex;gap:8px;align-items:center;margin-bottom:6px}
  .lenscard .title{font-weight:700}
  .lenscard .desc{color:var(--muted);font-size:14px;margin-bottom:8px}
  .lenscard .pills{display:flex;flex-wrap:wrap;gap:8px}
  .pill{border:1px solid var(--ring);background:#fff;border-radius:999px;padding:6px 10px;font-size:13px;cursor:pointer}
  .pill:hover{border-color:var(--uiuc-orange)}
  /* native title tooltip only */

  .dot{width:10px;height:10px;border-radius:999px;display:inline-block}
  .blue{background:var(--blue)} .green{background:var(--green)} .amber{background:var(--amber)} .red{background:var(--red)}

  /* Table */
  .toolbar{display:flex;flex-wrap:wrap;gap:8px;padding:12px 16px;border-top:1px solid var(--ring)}
  .toolbar input[type="search"]{flex:1;min-width:180px;padding:8px 10px;border:1px solid var(--ring);border-radius:10px}
  .toolbar select,.toolbar button{padding:8px 10px;border:1px solid var(--ring);border-radius:10px;background:#fff;cursor:pointer}
  .toolbar .seg{display:flex;gap:8px;flex-wrap:wrap}
  table{width:100%;border-collapse:collapse}
  thead th{position:sticky;top:0;background:#f3f4f6;font-weight:600;font-size:14px;color:#374151;border-bottom:1px solid var(--ring);padding:10px;text-align:left;cursor:pointer}
  tbody td{border-bottom:1px solid var(--ring);padding:10px;vertical-align:top;font-size:14px}
  tbody tr:hover{background:#fafafa}
  .tag{display:inline-block;margin:2px 6px 2px 0;padding:2px 8px;border-radius:999px;background:#eef2ff;color:#3730a3;font-size:12px;border:1px solid #e5e7eb}
  .mod{display:inline-block;margin:2px 6px 2px 0;padding:2px 8px;border-radius:999px;background:#fff7ed;color:#9a3412;font-size:12px;border:1px solid #fed7aa}
  .footer{padding:10px 16px;color:var(--muted);font-size:12px}

  /* Quiz */
  .quiz{padding:12px 16px 16px}
  .q{border-top:1px solid var(--ring);padding:12px 0}
  .q h4{margin:0 0 8px 0;font-size:15px}
  .choices{display:flex;flex-direction:column;gap:6px}
  .choice{display:flex;gap:8px;align-items:flex-start;padding:6px 8px;border:1px solid var(--ring);border-radius:10px;background:#fff;cursor:pointer;transition:border-color .12s, background .12s}
  .choice input{margin-top:2px}
  .choice.correct{border-color:#16a34a;background:#ecfdf5}
  .choice.incorrect{border-color:#dc2626;background:#fef2f2}
  .btn{display:inline-block;border:1px solid var(--uiuc-navy);color:#fff;background:var(--uiuc-navy);border-radius:999px;padding:8px 14px;cursor:pointer}
  .btn.alt{background:#fff;color:var(--uiuc-navy)}
  .score{font-weight:700}
</style>
</head>
<body>
<div class="wrap">
  <header>
    <h1>AI Models: Four Lenses</h1>
    <p>A teaching tool to separate the general idea of an <em>AI model</em> from a useful taxonomy: <strong>Training</strong>, <strong>Architecture</strong>, <strong>Family</strong>, and <strong>Lineage</strong>. Tabs for an overview, a unified table (with CSV export), and a quiz focused on modalities & real tasks.</p>
  </header>

  <!-- TABS -->
  <nav class="tabs" id="tabs">
    <button class="tab active" data-tab="overview">Overview</button>
    <button class="tab" data-tab="table">Unified Table</button>
    <button class="tab" data-tab="quiz">Quiz</button>
  </nav>

  <!-- OVERVIEW (default visible without JS) -->
  <section class="panel active default-visible" id="panel-overview">
    <div class="card">
      <h2 style="padding-bottom:8px;">Lens Overview</h2>
      <p class="sub">Click a lens to jump to the table with that lens pre-filtered. Click a pill to search that term.</p>
      <div class="grid" id="overview-grid"></div>
      <div class="footer" style="padding:12px 16px;">
        Want details? Jump to the <a href="#table" onclick="window.goTab && goTab('table')">Unified Table</a> or try the <a href="#quiz" onclick="window.goTab && goTab('quiz')">Quiz</a>.
      </div>
    </div>
  </section>

  <!-- TABLE -->
  <section class="panel" id="panel-table">
    <div class="card">
      <h2 style="padding-bottom:0;">Unified Table</h2>
      <p class="sub">Sortable, filterable across lenses; export CSV of the current view or the full dataset.</p>
      <div class="toolbar">
        <input id="table-search" type="search" placeholder="Search all columns…" aria-label="Search table"/>
        <select id="modality-filter" aria-label="Filter by modality">
          <option value="">All modalities</option>
          <option>Text</option><option>Image</option><option>Video</option>
          <option>Audio</option><option>Vision</option><option>Multimodal</option><option>Tabular</option>
        </select>
        <div class="seg" id="lens-filters"></div>
        <button id="export-current">Export CSV (current view)</button>
        <button id="export-all" class="alt">Export CSV (entire dataset)</button>
      </div>
      <div style="overflow:auto; max-height: 520px;">
        <table id="data-table">
          <thead>
            <tr>
              <th data-key="lens">Lens</th>
              <th data-key="category">Category</th>
              <th data-key="term">Term</th>
              <th data-key="modality">Modality</th>
              <th data-key="examples">Examples</th>
              <th data-key="explain">Layman explanation</th>
            </tr>
          </thead>
          <tbody></tbody>
        </table>
      </div>
      <div class="footer">Tip: click column headers to sort. Shift-click to multi-sort.</div>
    </div>
  </section>

  <!-- QUIZ -->
  <section class="panel" id="panel-quiz">
    <div class="card">
      <h2 style="padding-bottom:0;">Quiz — Modalities & Uses</h2>
      <p class="sub">Instant feedback. Running tally updates as you answer. “New quiz” shuffles.</p>
      <div class="quiz">
        <div id="quiz-box"></div>
        <div style="display:flex;gap:8px;margin-top:12px;align-items:center;">
          <button class="btn" id="quiz-new">New quiz</button>
          <div class="score" id="quiz-tally" style="margin-left:auto;">Correct: 0 · Attempted: 0</div>
        </div>
      </div>
    </div>
  </section>
</div>

<script>
/* ---------------- DATA (edit me) ----------------
   Each term is [term, explain, examples, modality]
*/
const DATA = {
  lenses: {
    Training: {
      color:"#2563eb",
      desc:"How does the model learn? Strategy used to fit parameters.",
      groups:[
        { name:"Supervised", color:"blue", terms:[
          ["Logistic Regression","Learns from labeled examples to predict categories.","Email spam filters","Tabular"],
          ["Random Forest","Ensemble of decision trees voting to reduce overfitting.","Risk scoring on tabular data","Tabular"],
          ["Fine-tuning (pretrained backbone)","Start from a pretrained model and teach a labeled task.","BERT/GPT fine-tunes for classification","Text"]
        ]},
        { name:"Unsupervised", color:"amber", terms:[
          ["K-Means","Clusters similar items without labels.","Customer segments","Tabular"],
          ["PCA","Projects data into fewer directions preserving variance.","Compression/visualization","Tabular"],
          ["Autoencoder","Compresses then reconstructs to reveal structure.","Anomaly detection","Vision"]
        ]},
        { name:"Reinforcement", color:"green", terms:[
          ["DQN","Learns actions via rewards.","Atari agents","Multimodal"],
          ["AlphaZero-style","Self-play + tree search.","Chess/Go engines","Multimodal"],
          ["RLHF phase","Human preference signals shape outputs after pretraining.","Chat alignment","Text"]
        ]},
        { name:"Self-supervised", color:"blue", terms:[
          ["Masked LM","Predict masked tokens to learn language structure.","BERT pretraining","Text"],
          ["Next-token prediction","Autoregressively predict the next token.","GPT/Mistral/Gemma pretraining","Text"],
          ["Contrastive learning","Pull same content together, push different apart.","CLIP, SimCLR for vision","Vision"]
        ]}
      ]
    },
    Architecture: {
      color:"#16a34a",
      desc:"What is the structural form of the model?",
      groups:[
        { name:"Classical", color:"amber", terms:[
          ["Decision Tree","If-then splits like a flowchart.","Explainable baselines","Tabular"],
          ["Linear / Logistic Regression","Weighted sums; fast/strong baselines.","Tabular tasks","Tabular"]
        ]},
        { name:"Neural Nets", color:"green", terms:[
          ["CNN","Sliding filters capture spatial patterns.","Vision tasks","Vision"],
          ["RNN / LSTM","Token-by-token with memory state.","Time-series, early NLP","Text"],
          ["Transformer (Encoder/Decoder)","Attention relates all tokens at once.","Modern LMs & some vision/audio","Multimodal"],
          ["State-Space Models (SSM) / Mamba","Sequence models with linear-time scaling.","Long-sequence efficiency","Text"]
        ]},
        { name:"Generative", color:"blue", terms:[
          ["Autoregressive Decoders","Generate one token/pixel at a time.","GPT-style LMs, PixelCNN","Multimodal"],
          ["Autoencoder / VAE","Compress→reconstruct to learn latent space.","Denoising, representation learning","Vision"],
          ["Diffusion (DDPM/Score-based)","Denoise noise step-by-step to sample data.","Stable Diffusion, Imagen, Sora (under the hood)","Image"],
          ["Latent Diffusion","Run diffusion in a learned latent space for speed.","SDXL / SD3","Image"],
          ["Flow-based Models","Invertible transforms with exact likelihoods.","RealNVP, Glow","Image"]
        ]}
      ]
    },
    Family: {
      color:"#f59e0b",
      desc:"Which named family or ecosystem does a model belong to?",
      groups:[
        { name:"OpenAI", color:"amber", terms:[
          ["GPT family","Autoregressive LLMs (e.g., GPT-3.5, GPT-4, o-series).","Chat, tools, code","Text"],
          ["DALL·E family","Text-to-image diffusion/AR hybrids.","DALL·E 2, DALL·E 3","Image"],
          ["Sora (video)","Text-to-video generative model (diffusion-like).","Video synthesis","Video"]
        ]},
        { name:"Anthropic", color:"blue", terms:[
          ["Claude family","Safety-aligned LLMs with long context.","Opus, Sonnet, Haiku","Text"]
        ]},
        { name:"Google / DeepMind", color:"blue", terms:[
          ["Gemini family","Multimodal successors to PaLM.","Text-image-audio-tools","Multimodal"],
          ["Imagen family","High-fidelity image diffusion.","Imagen / Imagen 2","Image"],
          ["Veo (video)","Text-to-video generation.","Video synthesis","Video"]
        ]},
        { name:"Meta", color:"green", terms:[
          ["LLaMA family","Open(-ish) LLM line with many community variants.","LLaMA-2/3","Text"],
          ["SAM family","Promptable segmentation models.","Segment Anything","Vision"]
        ]},
        { name:"Mistral AI (open-source)", color:"green", terms:[
          ["Mistral family","Efficient small/medium LLMs.","Mistral 7B","Text"],
          ["Mixtral family","Mixture-of-Experts (MoE) LLMs.","Mixtral 8×7B, 8×22B","Text"]
        ]},
        { name:"Alibaba", color:"green", terms:[
          ["Qwen family","Open LLMs across sizes and modalities.","Qwen 2/2.5, Qwen-VL","Multimodal"]
        ]},
        { name:"Databricks", color:"green", terms:[
          ["DBRX family","Mixture-of-Experts LLM.","DBRX Instruct","Text"]
        ]},
        { name:"Microsoft", color:"green", terms:[
          ["Phi family","Small, data-curated LLMs for efficiency.","Phi-2, Phi-3, Phi-4 mini","Text"]
        ]},
        { name:"xAI", color:"green", terms:[
          ["Grok family","LLMs with tool use emphasis.","Grok-1.x","Text"]
        ]},
        { name:"01.AI", color:"green", terms:[
          ["Yi family","High-quality open LLMs.","Yi-34B, Yi-1.5","Text"]
        ]},
        { name:"Cohere", color:"green", terms:[
          ["Command family","Enterprise-oriented LLMs.","Command-R / R+","Text"]
        ]},
        { name:"Open-source Vision/Multimodal", color:"green", terms:[
          ["Stable Diffusion family","Latent diffusion for images.","SD 1.5, SD 2.1, SDXL, SD3","Image"],
          ["CLIP / OpenCLIP","Text-image embedding models.","Retrieval, alignment","Multimodal"],
          ["DINOv2","Self-supervised vision backbone.","Feature extraction","Vision"]
        ]},
        { name:"Independent Audio/Video (closed)", color:"amber", terms:[
          ["Midjourney","Proprietary TTI (diffusion/AR stack).","MJ v1–v6","Image"],
          ["Runway Gen family","Text-to-video & image tools.","Gen-1/2/3","Video"],
          ["Pika","Text-to-video/image.","Pika 1.0/1.5","Video"],
          ["Suno / Udio","Text-to-music audio models.","Song generation","Audio"]
        ]}
      ]
    },
    Lineage: {
      color:"#dc2626",
      desc:"How versions evolve: releases, forks, and inherited innovations.",
      groups:[
        { name:"OpenAI (LLM & media)", color:"red", terms:[
          ["GPT-2 → GPT-3 → 3.5 → 4 → o-series","Scaling + instruction tuning + tool use.","Reasoning & safety improve","Text"],
          ["DALL·E 1 → 2 → 3","From discrete to high-fidelity diffusion.","Better text rendering","Image"],
          ["Sora (initial)","First public video model iteration.","Longer, coherent clips","Video"]
        ]},
        { name:"Meta", color:"green", terms:[
          ["LLaMA 1 → 2 → 3","Open weights and community fine-tunes.","Ecosystem acceleration","Text"]
        ]},
        { name:"Google / DeepMind", color:"blue", terms:[
          ["PaLM → PaLM-2 → Gemini","Shift to multimodality + efficiency.","Tool use & long context","Multimodal"],
          ["Imagen → Imagen 2 → Veo","Image fidelity → video generation.","Photorealism & motion","Image"]
        ]},
        { name:"Mistral", color:"green", terms:[
          ["Mistral 7B → Mixtral 8×7B → 8×22B","From dense to MoE scaling.","Latency/quality trade-offs","Text"]
        ]},
        { name:"Alibaba", color:"green", terms:[
          ["Qwen 1 → 2 → 2.5","Coverage, multilinguality, tools; VL/Audio variants.","","Multimodal"]
        ]},
        { name:"Databricks", color:"green", terms:[
          ["DBRX base → DBRX instruct","Open MoE tuned for RAG.","Enterprise workflows","Text"]
        ]},
        { name:"Microsoft", color:"green", terms:[
          ["Phi-1 → 2 → 3 → 4-mini","Data-centric small LLMs.","Edge & low-cost use","Text"]
        ]},
        { name:"01.AI", color:"green", terms:[
          ["Yi 34B → Yi-1.5","Open large models tuned for quality.","","Text"]
        ]},
        { name:"Stable Diffusion (Stability/CompVis)", color:"blue", terms:[
          ["SD 1.x → SD 2.x → SDXL → SD3","Latent diffusion, larger backbones.","Better text rendering","Image"]
        ]},
        { name:"Midjourney (closed)", color:"amber", terms:[
          ["v1 → v2 → v3 → v4 → v5 → v6","Proprietary stack, strong style control.","Artistic TTI","Image"]
        ]},
        { name:"Video Gen (mixed)", color:"amber", terms:[
          ["Runway Gen-1 → Gen-2 → Gen-3","Text-to-video quality & temporal coherence.","Short clips","Video"],
          ["Pika 1.0 → 1.5","Fast iteration on video.","Animated assets","Video"]
        ]},
        { name:"Emerging Open LLMs", color:"green", terms:[
          ["DeepSeek V2 → V2.5 → V3","Training efficiency & reasoning focus.","","Text"]
        ]}
      ]
    }
  }
};

/* ---------------- Helpers ---------------- */
const $ = s=>document.querySelector(s);
const $$ = s=>Array.from(document.querySelectorAll(s));
function el(tag, attrs={}, ...children){
  const n=document.createElement(tag);
  for(const [k,v] of Object.entries(attrs)){
    if(k==="class") n.className=v;
    else if(k==="html") n.innerHTML=v;
    else n.setAttribute(k,v);
  }
  children.flat().forEach(c=>n.append(c));
  return n;
}
function debounce(fn,ms=200){let t;return(...a)=>{clearTimeout(t);t=setTimeout(()=>fn(...a),ms)}}
function shuffle(x){ const a=[...x]; for(let i=a.length-1;i>0;i--){const j=Math.floor(Math.random()*(i+1)); [a[i],a[j]]=[a[j],a[i]];} return a; }
function pick(n,arr){ const a=[...arr]; const out=[]; while(out.length<n && a.length){ out.push(a.splice(Math.floor(Math.random()*a.length),1)[0]); } return out; }

/* ---------------- Tabs ---------------- */
function goTab(name){
  const tabs=$$("#tabs .tab"); const panels=$$(".panel");
  tabs.forEach(b=>b.classList.toggle("active", b.dataset.tab===name));
  panels.forEach(p=>p.classList.toggle("active", p.id===`panel-${name}`));
  if(name==="table"){ const t=$("#table-search"); t && t.focus(); }
  history.replaceState(null,"",`#${name}`);
}

/* ---------------- Overview ---------------- */
function lensRows(){
  const rows=[];
  Object.entries(DATA.lenses).forEach(([lens,meta])=>{
    meta.groups.forEach(g=>{
      g.terms.forEach(([term,explain,examples,mod])=>{
        rows.push({lens,category:g.name,term,explain,examples,mod});
      });
    });
  });
  return rows;
}
const ALL_ROWS = lensRows();

function renderOverview(){
  const grid=$("#overview-grid"); if(!grid) return;
  grid.innerHTML="";
  const lensOrder=["Training","Architecture","Family","Lineage"];
  lensOrder.forEach(name=>{
    const meta=DATA.lenses[name];
    const colorClass = name==="Training"?"blue":name==="Architecture"?"green":name==="Family"?"amber":"red";
    const rows = ALL_ROWS.filter(r=>r.lens===name);
    const topTerms = rows.slice(0,6);

    const card=el("div",{class:"lenscard card", style:`border-left-color:${meta.color}`});
    const head=el("div",{class:"row"},
      el("span",{class:`dot ${colorClass}`}),
      el("span",{class:"title"}, name),
      el("span",{style:"color:var(--muted);margin-left:auto;font-size:12px;"}, `${rows.length} terms`)
    );
    const desc=el("div",{class:"desc"}, meta.desc);
    const actions=el("div",{}, el("button",{class:"pill", title:`View ${name} in table`}, "Open in Table →"));
    actions.querySelector("button").addEventListener("click",()=>{
      goTab("table");
      $$("#lens-filters input[type=checkbox]").forEach(c=>{ c.checked = (c.value===name); });
      renderTable();
    });

    const pills=el("div",{class:"pills"});
    topTerms.forEa
